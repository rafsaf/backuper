{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Backuper","text":"<p>A tool for performing scheduled database backups and transferring encrypted data to secure clouds, for home labs, hobby projects, etc., in environments such as k8s, docker, vms.</p> WARNING While this project aims to be a reliable backup tool and can help protect your hobby 5GB Postgres database from evaporation, it is NOT suitable for enterprise production systems with huge databases and application workloads. You have been warned."},{"location":"#documentation","title":"Documentation","text":"<ul> <li>https://backuper.rafsaf.pl</li> </ul>"},{"location":"#supported-backup-targets","title":"Supported backup targets","text":"<ul> <li>PostgreSQL (tested on 15, 14, 13, 12, 11)</li> <li>MySQL (tested on 8.0, 5.7)</li> <li>MariaDB (tested on 10.11, 10.6, 10.5, 10.4)</li> <li>Single file</li> <li>Directory</li> </ul>"},{"location":"#supported-upload-providers","title":"Supported upload providers","text":"<ul> <li>Google Cloud Storage bucket</li> <li>Debug (local)</li> </ul>"},{"location":"#notifications","title":"Notifications","text":"<ul> <li>Discord</li> </ul>"},{"location":"#deployment-strategies","title":"Deployment strategies","text":"<p>Dockerhub: https://hub.docker.com/r/rafsaf/backuper</p> <ul> <li>docker (docker compose) container</li> <li>kubernetes deployment</li> </ul>"},{"location":"#architectures","title":"Architectures","text":"<ul> <li>linux/amd64</li> <li>linux/arm64</li> </ul>"},{"location":"#example","title":"Example","text":"<p>Everyday 5am backup to Google Cloud Storage of PostgreSQL database defined in the same file and running in docker container.</p> <pre><code># docker-compose.yml\n\nservices:\n  db:\n    image: postgres:15\n    environment:\n      - POSTGRES_PASSWORD=pwd\n  backuper:\n    image: rafsaf/backuper:latest\n    environment:\n      - POSTGRESQL_PG15=host=db password=pwd cron_rule=0 0 5 * * port=5432\n      - ZIP_ARCHIVE_PASSWORD=change_me\n      - BACKUP_PROVIDER=name=debug\n</code></pre> <p>(NOTE this will use provider debug that store backups locally in the container).</p> <p> </p>"},{"location":"configuration/","title":"Configuration","text":"<p>Environemt variables</p> Name Type Description Default ZIP_ARCHIVE_PASSWORD string[required] Zip archive password that all backups generated by this backuper instance will have. When it is lost, you lose access to your backups. - BACKUP_PROVIDER string[required] See <code>Providers</code> chapter, choosen backup provider for example GCS. - BACKUP_MAX_NUMBER int How many backups can live at once for backup target. Defaults to <code>7</code>. Note this must makes sense with cron expression you use. For example if you want to have 7 day retention, and make backups at 5:00, BACKUP_MAX_NUMBER=7 is fine, but if you make 4 backups per day, you will need BACKUP_MAX_NUMBER=28. 7 POSTGRESQL_... backup target syntax PostgreSQL database target, see PostgreSQL. - MYSQL_... backup target syntax MySQL database target, see MySQL. - MARIADB_... backup target syntax MariaDB database target, see MariaDB. - SINGLEFILE_... backup target syntax Single file database target, see Single file. - DIRECTORY_... backup target syntax Directory database target, see Directory. - DISCORD_SUCCESS_WEBHOOK_URL url URL for success messages. - DISCORD_FAIL_WEBHOOK_URL url URL for fail messages. - DISCORD_NOTIFICATION_MAX_MSG_LEN int Maximum length of messages send to discord API. 1500 LOG_LEVEL string Case sensitive const log level, must be one of <code>INFO</code>, <code>DEBUG</code>, <code>WARNING</code>, <code>ERROR</code>. INFO SUBPROCESS_TIMEOUT_SECS int Indicates how long subprocesses can last. Note that all backups are run from shell in subprocesses. Defaults to 3600 seconds which should be enough for even big dbs to make backup of. 3600 ZIP_ARCHIVE_LEVEL int[1-9] Compression level of 7-zip via <code>-mx</code> option: <code>-mx[N] : set compression level: -mx1 (fastest) ... -mx9 (ultra)</code>. Defaults to <code>3</code> which should be sufficient and fast enough. 3 LOG_FOLDER_PATH string Path to store log files, for local development <code>./logs</code>, in container <code>/var/log/backuper</code>. /var/log/backuper SIGTERM_TIMEOUT_SECS int Time in seconds on exit how long backuper will wait for ongoing backup threads before force killing them and exiting. 30 <p> </p>"},{"location":"deployment/","title":"Deployment","text":"<p>In general, use docker image <code>rafsaf/backuper</code> (here available tags on dockerhub), it supports both <code>amd64</code> and <code>arm64</code> architectures. Standard way would be to run it with docker compose or as a kubernetes deployment. If not sure, use <code>latest</code>.</p>"},{"location":"deployment/#docker-compose","title":"Docker Compose","text":""},{"location":"deployment/#docker-compose-file","title":"Docker compose file","text":"<pre><code># docker-compose.yml\n\nservices:\n  backuper:\n    container_name: backuper\n    image: rafsaf/backuper:latest\n    environment:\n      - POSTGRESQL_DB1=...\n      - MYSQL_DB2=...\n      - MARIADB_DB3=...\n\n      - ZIP_ARCHIVE_PASSWORD=change_me\n      - BACKUP_PROVIDER=name=gcs bucket_name=my_bucket_name bucket_upload_path=my_backuper_instance_1 service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo=\n</code></pre>"},{"location":"deployment/#notes","title":"Notes","text":"<ul> <li>For hard debug you can set <code>LOG_LEVEL=DEBUG</code> and use (container name is backuper):   <pre><code>docker logs backuper\n</code></pre></li> <li>There is dedicated flag --single that ignores cron, make all databases backups and exits. To use it when having already running container, use:   <pre><code>docker compose run --rm backuper python -m backuper.main --single\n</code></pre>   BE CAREFUL, if your setup if fine, this will upload backup files to cloud provider, so costs may apply.</li> </ul>"},{"location":"deployment/#kubernetes","title":"Kubernetes","text":"<pre><code># backuper-deployment.yml\n\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: backuper\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: backuper-secrets\n  namespace: backuper\ntype: Opaque\nstringData:\n  POSTGRESQL_DB1: ...\n  MYSQL_DB2: ...\n  MARIADB_DB3: ...\n  ZIP_ARCHIVE_PASSWORD: change_me\n  BACKUP_PROVIDER: \"name=gcs bucket_name=my_bucket_name bucket_upload_path=my_backuper_instance_1 service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo=\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: backuper\n  name: backuper\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: backuper\n  template:\n    metadata:\n      labels:\n        app: backuper\n    spec:\n      containers:\n        - image: rafsaf/backuper:latest\n          name: backuper\n          envFrom:\n            - secretRef:\n                name: backuper-secrets\n</code></pre>"},{"location":"deployment/#notes_1","title":"Notes","text":"<ul> <li>For hard debug you can set <code>LOG_LEVEL: DEBUG</code> and use (for brevity random pod name used):   <pre><code>kubectl logs backuper-9c8b8b77d-z5xsc -n backuper\n</code></pre></li> <li>There is dedicated flag --single that ignores cron, make all databases backups and exits. To use it when having already running container, use:   <pre><code>kubectl exec --stdin --tty backuper-9c8b8b77d-z5xsc -n backuper -- runuser -u backuper -- python -m backuper.main --single\n</code></pre>   BE CAREFUL, if your setup if fine, this will upload backup files to cloud provider, so costs may apply.</li> </ul>"},{"location":"how_to_restore/","title":"How to restore","text":"<p>To restore backups you already have in cloud, for sure you will need <code>7-zip</code>, <code>unzip</code> or equivalent software to unzip the archive (and of course password <code>ZIP_ARCHIVE_PASSWORD</code> used for creating it in a first place). That step is ommited below.</p> <p>For below databases restore, you can for sure use <code>backuper</code> image itself (as it already has required software installed, for restore also, and must have network access to database). Tricky part can be \"how to deliver zipped backup file to backuper container\". This is also true for transporting it anywhere. Usual way is to use <code>scp</code> and for containers for docker compose and kubernetes respectively <code>docker compose cp</code> and <code>kubectl cp</code>. </p> <p>Other idea if you feel unhappy with passing your database backups around (even if password protected) would be to make the backup file public for a moment and available to download and use tools like <code>curl</code> to download it on destination place. If leaked, there is yet very strong cryptography to protect you. This should be sufficient for bunch of projects.</p>"},{"location":"how_to_restore/#directory-and-single-file","title":"Directory and single file","text":"<p>Just file or directory, copy them back where you want.</p>"},{"location":"how_to_restore/#postgresql","title":"PostgreSQL","text":"<p>Backup is made using <code>pg_dump</code> (see def _backup() params). To restore database, you will need <code>psql</code> https://www.postgresql.org/docs/current/app-psql.html and network access to database. If on debian/ubuntu, this is provided by apt package <code>postgresql-client</code>.</p> <p>Follow docs (backuper creates typical SQL file backups, nothing special about them), but command will look something like that:</p> <pre><code>psql -h localhost -p 5432 -U postgres database_name -W &lt; backup_file.sql\n</code></pre>"},{"location":"how_to_restore/#mysql","title":"MySQL","text":"<p>Backup is made using <code>mysqldump</code> (see def _backup() params). To restore database, you will need <code>mysql</code> https://dev.mysql.com/doc/refman/8.0/en/mysql.html and network access to database. If on debian/ubuntu, this is provided by apt package <code>mysql-client</code>.</p> <p>Follow docs (backuper creates typical SQL file backups, nothing special about them), but command will look something like that:</p> <pre><code>mysql -h localhost -P 3306 -u root -p database_name &lt; backup_file.sql\n</code></pre>"},{"location":"how_to_restore/#mariadb","title":"MariaDB","text":"<p>Backup is made using <code>mariadb-dump</code> (see def _backup() params). To restore database, you will need <code>mysql</code> or <code>mariadb</code> https://dev.mysql.com/doc/refman/8.0/en/mysql.html or https://mariadb.com/kb/en/mariadb-command-line-client/ and network access to database. If on debian/ubuntu, this is provided by apt package <code>mysql-client</code> or see https://mariadb.com/kb/en/mariadb-package-repository-setup-and-usage/.</p> <p>Follow docs (backuper creates typical SQL file backups, nothing special about them), but command will look something like that:</p> <pre><code>mariadb -h localhost -P 3306 -u root -p database_name &lt; backup_file.sql\n</code></pre> <p> </p>"},{"location":"backup_targets/directory/","title":"Directory","text":""},{"location":"backup_targets/directory/#environment-variable","title":"Environment variable","text":"<pre><code>DIRECTORY_SOME_STRING=\"abs_path=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"DIRECTORY_\" will be handled as Directory. There can be multiple files paths definition for one backuper instance, for example <code>DIRECTORY_FOO</code> and <code>DIRECTORY_BAR</code>. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/directory/#params","title":"Params","text":"Name Type Description Default abs_path string[requried] Absolute path to folder for backup. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - max_backups int Max number of backups stored in upload provider, if this number is exceeded, oldest one is removed, by default enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER"},{"location":"backup_targets/directory/#examples","title":"Examples","text":"<pre><code># 1. Directory /home/user/folder with backup every single minute\nDIRECTORY_FIRST='abs_path=/home/user/folder cron_rule=* * * * *'\n\n# 2. Directory /etc with backup on every night (UTC) at 05:00\nDIRECTORY_SECOND='abs_path=/etc cron_rule=0 5 * * *'\n\n# 3. Mounted directory /mnt/homedir with backup on every 6 hours at '15 with max number of backups of 20\nDIRECTORY_HOME_DIR='abs_path=/mnt/homedir cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"backup_targets/file/","title":"Single file","text":""},{"location":"backup_targets/file/#environment-variable","title":"Environment variable","text":"<pre><code>SINGLEFILE_SOME_STRING=\"abs_path=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"SINGLEFILE_\" will be handled as Single File. There can be multiple files paths definition for one backuper instance, for example <code>SINGLEFILE_FOO</code> and <code>SINGLEFILE_BAR</code>. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/file/#params","title":"Params","text":"Name Type Description Default abs_path string[requried] Absolute path to file for backup. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - max_backups int Max number of backups stored in upload provider, if this number is exceeded, oldest one is removed, by default enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER"},{"location":"backup_targets/file/#examples","title":"Examples","text":"<pre><code># File /home/user/file.txt with backup every single minute\nSINGLEFILE_FIRST='abs_path=/home/user/file.txt cron_rule=* * * * *'\n\n# File /etc/hosts with backup on every night (UTC) at 05:00\nSINGLEFILE_SECOND='abs_path=/etc/hosts cron_rule=0 5 * * *'\n\n# File config.json in mounted dir /mnt/appname with backup on every 6 hours at '15 with max number of backups of 20\nSINGLEFILE_THIRD='abs_path=/mnt/appname/config.json cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"backup_targets/mariadb/","title":"MariaDB","text":""},{"location":"backup_targets/mariadb/#environment-variable","title":"Environment variable","text":"<pre><code>MARIADB_SOME_STRING=\"host=... password=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"MARIADB_\" will be handled as MariaDB. There can be multiple files paths definition for one backuper instance, for example <code>MARIADB_FOO_MY_DB1</code> and <code>MARIADB_BAR_MY_DB2</code>. Supported versions are: 10.11, 10.6, 10.5, 10.4. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/mariadb/#params","title":"Params","text":"Name Type Description Default password string[requried] Mariadb database password. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - user string Mariadb database username. root host string Mariadb database hostname. localhost port int Mariadb database port. 3306 db string Mariadb database name. mariadb max_backups int Max number of backups stored in upload provider, if this number is exceeded, oldest one is removed, by default enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER"},{"location":"backup_targets/mariadb/#examples","title":"Examples","text":"<pre><code># 1. Local MariaDB with backup every single minute\nMARIADB_FIRST_DB='host=localhost port=3306 password=secret cron_rule=* * * * *'\n\n# 2. MariaDB in local network with backup on every night (UTC) at 05:00\nMARIADB_SECOND_DB='host=10.0.0.1 port=3306 user=foo password=change_me! db=bar cron_rule=0 5 * * *'\n\n# 3. MariaDB in local network with backup on every 6 hours at '15 with max number of backups of 20\nMARIADB_THIRD_DB='host=192.168.1.5 port=3306 user=root password=change_me_please! db=project cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"backup_targets/mysql/","title":"MySQL","text":""},{"location":"backup_targets/mysql/#environment-variable","title":"Environment variable","text":"<pre><code>MYSQL_SOME_STRING=\"host=... password=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"MYSQL_\" will be handled as MySQL. There can be multiple files paths definition for one backuper instance, for example <code>MYSQL_FOO_MY_DB1</code> and <code>MYSQL_BAR_MY_DB2</code>. Supported versions are: 8.0, 5.7. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/mysql/#params","title":"Params","text":"Name Type Description Default password string[requried] MySQL database password. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - user string MySQL database username. root host string MySQL database hostname. localhost port int MySQL database port. 3306 db string MySQL database name. mysql max_backups int Max number of backups stored in upload provider, if this number is exceeded, oldest one is removed, by default enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER"},{"location":"backup_targets/mysql/#examples","title":"Examples","text":"<pre><code># 1. Local MySQL with backup every single minute\nMYSQL_FIRST_DB='host=localhost port=3306 password=secret cron_rule=* * * * *'\n\n# 2. MySQL in local network with backup on every night (UTC) at 05:00\nMYSQL_SECOND_DB='host=10.0.0.1 port=3306 user=foo password=change_me! db=bar cron_rule=0 5 * * *'\n\n# 3. MySQL in local network with backup on every 6 hours at '15 with max number of backups of 20\nMYSQL_THIRD_DB='host=192.168.1.5 port=3306 user=root password=change_me_please! db=project cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"backup_targets/postgresql/","title":"PostgreSQL","text":""},{"location":"backup_targets/postgresql/#environment-variable","title":"Environment variable","text":"<pre><code>POSTGRESQL_SOME_STRING=\"host=... password=... cron_rule=...\"\n</code></pre> <p>Note</p> <p>Any environment variable that starts with \"POSTGRESQL_\" will be handled as PostgreSQL. There can be multiple files paths definition for one backuper instance, for example <code>POSTGRESQL_FOO_MY_DB1</code> and <code>POSTGRESQL_BAR_MY_DB2</code>. Supported versions are: 15, 14, 13, 12, 11. Params must be included in value, splited by single space for example <code>\"value1=1 value2=foo\"</code>.</p>"},{"location":"backup_targets/postgresql/#params","title":"Params","text":"Name Type Description Default password string[requried] PostgreSQL database password. - cron_rule string[requried] Cron expression for backups, see https://crontab.guru/ for help. - user string PostgreSQL database username. postgres host string PostgreSQL database hostname. localhost port int PostgreSQL database port. 5432 db string PostgreSQL database name. postgres max_backups int Max number of backups stored in upload provider, if this number is exceeded, oldest one is removed, by default enviornment variable BACKUP_MAX_NUMBER, see Configuration. BACKUP_MAX_NUMBER"},{"location":"backup_targets/postgresql/#examples","title":"Examples","text":"<pre><code># 1. Local PostgreSQL with backup every single minute\nPOSTGRESQL_FIRST_DB='host=localhost port=5432 password=secret cron_rule=* * * * *'\n\n# 2. PostgreSQL in local network with backup on every night (UTC) at 05:00\nPOSTGRESQL_SECOND_DB='host=10.0.0.1 port=5432 user=foo password=change_me! db=bar cron_rule=0 5 * * *'\n\n# 3. PostgreSQL in local network with backup on every 6 hours at '15 with max number of backups of 20\nPOSTGRESQL_THIRD_DB='host=192.168.1.5 port=5432 user=root password=change_me_please! db=project cron_rule=15 */3 * * * max_backups=20'\n</code></pre>"},{"location":"notifications/discord/","title":"Discord","text":"<p>It is possible to send messages to your Discord channels in events of success or/and failed backups. Good idea might be to have one muted and one normal channel on your server so success messages will be printed to muted one and fail messages to one not muted.</p> <p>Integration is via Discord webhooks and environment variables.</p> <p>Follow their documentation https://support.discord.com/hc/en-us/articles/228383668-Intro-to-Webhooks.</p> <p>You should generate webhooks like <code>\"https://discord.com/api/webhooks/1111111111/long-token\"</code> and <code>\"https://discord.com/api/webhooks/22222222222222/another-long-token\"</code> this way</p>"},{"location":"notifications/discord/#environemt-variables","title":"Environemt variables","text":"Name Type Description Default DISCORD_SUCCESS_WEBHOOK_URL url URL for success messages. - DISCORD_FAIL_WEBHOOK_URL url URL for fail messages. - DISCORD_NOTIFICATION_MAX_MSG_LEN int Maximum length of messages send to discord API. 1500"},{"location":"notifications/discord/#examples","title":"Examples:","text":"<pre><code>DISCORD_SUCCESS_WEBHOOK_URL=\"https://discord.com/api/webhooks/1111111111/long-token\"\nDISCORD_FAIL_WEBHOOK_URL=\"https://discord.com/api/webhooks/22222222222222/another-long-token\"\n</code></pre>"},{"location":"other/development/","title":"Development","text":"<p>General knowledge of Git, Python and Docker is assumed.</p>"},{"location":"other/development/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11</li> <li>Poetry https://python-poetry.org/</li> <li>Docker and docker compose plugin https://docs.docker.com/get-docker/</li> <li>Debian/Ubuntu</li> </ul>"},{"location":"other/development/#setup-steps","title":"Setup steps","text":"<ol> <li> <p>Install python dependencies</p> <p><code>poetry install</code></p> </li> <li> <p>Create <code>.env</code> file</p> <p><code>cp .env.example .env</code></p> </li> <li> <p>To run database backups, you will need mariadb-client and postgresql-client installed and copied good version of 7zip (arm64 or amd64) from bin folder, there are dedicated scripts in folder <code>script</code> that does that.</p> </li> <li> <p>Setup databases</p> <p><code>docker compose up -d postgres_15 postgres_14 postgres_13 postgres_12 postgres_11 mysql_57 mysql_80 mariadb_1011 mariadb_1006 mariadb_1005 mariadb_1004</code></p> </li> <li> <p>You can run backup and pytest tests (--single to make all backups immediatly and then exit)</p> <p><code>python -m backuper.main --single</code></p> <p><code>pytest</code></p> </li> </ol>"},{"location":"other/development/#docs","title":"Docs","text":"<p>To play with documentation, after dependencies are in place installed with poetry:</p> <p><code>mkdocs serve</code> will start development server.</p> <p> </p>"},{"location":"other/history/","title":"History","text":"<p>This project existance was directly caused by lack of tool like that in the wild for postgres database. At the time (mid 2022) there were some bash scripts that could perform backup, but their overall quality was not very high. I moved one hobbyst (but production! with real clients) few GB database from managed cloud solution to my own VM and knew backups are necessary in the long run.</p> <p>From there, after few iterations eventually I thought it would be also a good thing to backup some other projects in my k3s kubernetes cluster, and there are postgres, mysql and mariadb databases so decision to support all of them was natural.</p> <p>Tried to wrote it in extensible way so new backup targets like databases and cloud providers can be added.</p>"},{"location":"providers/debug/","title":"Debug","text":""},{"location":"providers/debug/#environment-variable","title":"Environment variable","text":"<pre><code>BACKUP_PROVIDER=\"name=debug\"\n</code></pre> <p>Uses only local files (folder inside container) for storing backup. This is meant only for debug purposes.</p> <p>If you absolutely must not upload backups to outside world, consider adding some persistant volume for folder where buckups live in the container, that is <code>/var/lib/backuper/data</code>.</p> <p>Note</p> <p>There can be only one upload provider defined per app, using BACKUP_PROVIDER environemnt variable. It's type is guessed by using <code>name</code>, in this case <code>name=debug</code>.</p>"},{"location":"providers/debug/#params","title":"Params","text":"Name Type Description Default name string[requried] Must be set literaly to string <code>debug</code> to use Debug. -"},{"location":"providers/debug/#examples","title":"Examples","text":"<pre><code># 1. Debug provider\nBACKUP_PROVIDER='name=debug'\n</code></pre>"},{"location":"providers/google_cloud_storage/","title":"Google Cloud Storage","text":""},{"location":"providers/google_cloud_storage/#environment-variable","title":"Environment variable","text":"<pre><code>BACKUP_PROVIDER=\"name=gcs bucket_name=my_bucket_name bucket_upload_path=my_backuper_instance_1 service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo=\"\n</code></pre> <p>Uses Google Cloud Storage bucket for storing backups.</p> <p>Note</p> <p>There can be only one upload provider defined per app, using BACKUP_PROVIDER environemnt variable. It's type is guessed by using <code>name</code>, in this case <code>name=gcs</code>. Params must be included in value, splited by single space for example \"value1=1 value2=foo\".</p>"},{"location":"providers/google_cloud_storage/#params","title":"Params","text":"Name Type Description Default name string[requried] Must be set literaly to string <code>gcs</code> to use Google Cloud Storage. - bucket_name string[requried] Your globally unique bucket name. - bucket_upload_path string[requried] Prefix that every created backup will have, for example if it is equal to <code>my_backuper_instance_1</code>, paths to backups will look like <code>my_backuper_instance_1/your_backup_target_eg_postgresql/file123.zip</code>. Usually this should be something unique for this backuper instance, for example <code>k8s_foo_backuper</code>. - service_account_base64 string[requried] Base64 JSON service account file created in IAM, with write and read access permissions to bucket, see Resources below. - chunk_size_mb int The size of a chunk of data transfered to GCS, consider lower value only if for example your internet connection is slow or you know what you are doing, 100MB is google default. 100 chunk_timeout_secs int The chunk of data transfered to GCS upload timeout, consider higher value only if for example your internet connection is slow or you know what you are doing, 60s is google default. 60"},{"location":"providers/google_cloud_storage/#examples","title":"Examples","text":"<pre><code># 1. Bucket pets-bucket\nBACKUP_PROVIDER='name=gcs bucket_name=pets-bucket bucket_upload_path=pets_backuper service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo='\n\n# 2. Bucket birds with smaller chunk size\nBACKUP_PROVIDER='name=gcs bucket_name=birds bucket_upload_path=birds_backuper chunk_size_mb=25 chunk_timeout_secs=120 service_account_base64=Z29vZ2xlX3NlcnZpY2VfYWNjb3VudAo='\n</code></pre>"},{"location":"providers/google_cloud_storage/#resources","title":"Resources","text":""},{"location":"providers/google_cloud_storage/#creating-bucket","title":"Creating bucket","text":"<p>https://cloud.google.com/storage/docs/creating-buckets</p>"},{"location":"providers/google_cloud_storage/#creating-service-account","title":"Creating service account","text":"<p>https://cloud.google.com/iam/docs/service-accounts-create</p>"},{"location":"providers/google_cloud_storage/#giving-it-required-roles-to-service-account","title":"Giving it required roles to service account","text":"<ol> <li> <p>Go \"IAM and admin\" -&gt; \"IAM\"</p> </li> <li> <p>Find your service account and update its roles</p> </li> </ol> <p>Give it following roles so it will have read access for whole bucket \"my_bucket_name\" and admin access for only path prefix \"my_backuper_instance_1\" in bucket \"my_bucket_name\":</p> <ol> <li>Storage Object Admin (with IAM condition: NAME starts with <code>projects/_/buckets/my_bucket_name/objects/my_backuper_instance_1</code>)</li> <li>Storage Object Viewer (with IAM condition: NAME starts with <code>projects/_/buckets/my_bucket_name</code>)</li> </ol> <p>After sucessfully creating service account, create new private key with JSON type and download it. File similar to <code>your_project_name-03189413be28.json</code> will appear in your Downloads.</p> <p>To get base64 (without any new lines) from it, use command:</p> <pre><code>cat your_project_name-03189413be28.json | base64 -w 0\n</code></pre>"},{"location":"providers/google_cloud_storage/#terraform","title":"Terraform","text":"<p>If using terraform for managing cloud infra, Service Accounts definition can be following:</p> <pre><code>resource \"google_service_account\" \"backuper-my_backuper_instance_1\" {\naccount_id   = \"backuper-my_backuper_instance_1\"\ndisplay_name = \"SA my_backuper_instance_1 for backuper bucket access\"\n}\n\nresource \"google_project_iam_member\" \"backuper-my_backuper_instance_1-iam-object-admin\" {\nproject = local.project_id\n  role    = \"roles/storage.objectAdmin\"\nmember  = \"serviceAccount:${google_service_account.backuper-my_backuper_instance_1.email}\"\ncondition {\ntitle      = \"object_admin_only_backuper_bucket_specific_path\"\nexpression = \"resource.name.startsWith(\\\"projects/_/buckets/my_bucket_name/objects/my_backuper_instance_1\\\")\"\n}\n}\nresource \"google_project_iam_member\" \"backuper-my_backuper_instance_1-iam-object-viewer\" {\nproject = local.project_id\n  role    = \"roles/storage.objectViewer\"\nmember  = \"serviceAccount:${google_service_account.backuper-my_backuper_instance_1.email}\"\n\ncondition {\ntitle      = \"object_viewer_only_backuper_bucket\"\nexpression = \"resource.name.startsWith(\\\"projects/_/buckets/my_bucket_name\\\")\"\n}\n}\n</code></pre> <p> </p>"}]}